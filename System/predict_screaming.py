# -*- coding: utf-8 -*-
"""Predict_Screaming.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ggwHKGOxp_IqCxbltHT8lhm9uAafn-FA
"""

import os
import torch
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
import scipy
import librosa
from torch import nn
from scipy.signal import stft
import sounddevice as sd

duration = 5  # 초
sampling_rate = 44100
print("Recording...")
audio_data = sd.rec(int(duration * sampling_rate), samplerate=sampling_rate, channels=1, dtype='float32')
sd.wait()
sf.write("recorded_audio.wav", audio_data, sampling_rate)
print("Recording saved.")

# 모델 정의 (CNN+Transformer)
class CNNTransformer(nn.Module):
    def __init__(self, input_size=257, num_classes=1):
        super(CNNTransformer, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv1d(input_size, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2),
            nn.Conv1d(128, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2)
        )
        self.transformer_layer = nn.TransformerEncoderLayer(d_model=32, nhead=2)
        self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=1)
        self.fc = nn.Linear(32, num_classes)

    def forward(self, x):
        x = self.cnn(x)
        x = x.permute(2, 0, 1)
        x = self.transformer(x)
        x = x.mean(dim=0)
        x = self.fc(x)
        return x

# 모델 불러오기
model = CNNTransformer(input_size=257, num_classes=1)
model.load_state_dict(torch.load("cnn_transformer_model.pth"))
model.eval()

# Fourier Transform 함수
def perform_fourier_transform(audio_data):
    _, _, freq_data = stft(audio_data, fs=sampling_rate, window='rectangular', nperseg=512, noverlap=0, nfft=512)
    magnitude = np.abs(freq_data)
    return magnitude

# WAV 파일을 사용한 판단 함수
def process_audio_file(file_path):
    print(f"Loading audio file: {file_path}")
    audio_data, sr = sf.read(file_path)
    if sr != sampling_rate:
        audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=sampling_rate)

    # Fourier Transform 및 데이터 변환
    magnitude = perform_fourier_transform(audio_data)
    magnitude = magnitude.T  # Transpose to match (time_frames, frequency_bins)
    input_data = torch.tensor(magnitude, dtype=torch.float32).unsqueeze(0).permute(0, 2, 1)  # (batch_size, channels, sequence_length)

    with torch.no_grad():
        output = model(input_data)
        prediction = torch.sigmoid(output).item()

    # 예측 결과 시각화
    plt.figure(figsize=(10, 6))

    # 오디오 신호 플롯
    plt.subplot(2, 1, 1)
    plt.plot(audio_data[:sampling_rate])
    plt.title("Audio Signal")
    plt.xlim(0, len(audio_data))
    plt.ylim(-1, 1)

    # 예측 결과 출력
    plt.subplot(2, 1, 2)
    plt.title("Prediction Result")
    plt.axis('off')

    plt.tight_layout()
    plt.show()

# 메인 함수 실행
if __name__ == "__main__":
    file_path = "/Users/iuseong/Downloads/Record/screaming001.wav"
    process_audio_file(file_path)